{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound2unicode(text):\n",
    "  #https://gist.github.com/redphx/9320735`\n",
    "  text = text.replace(\"\\u0065\\u0309\", \"\\u1EBB\")    # ẻ\n",
    "  text = text.replace(\"\\u0065\\u0301\", \"\\u00E9\")    # é\n",
    "  text = text.replace(\"\\u0065\\u0300\", \"\\u00E8\")    # è\n",
    "  text = text.replace(\"\\u0065\\u0323\", \"\\u1EB9\")    # ẹ\n",
    "  text = text.replace(\"\\u0065\\u0303\", \"\\u1EBD\")    # ẽ\n",
    "  text = text.replace(\"\\u00EA\\u0309\", \"\\u1EC3\")    # ể\n",
    "  text = text.replace(\"\\u00EA\\u0301\", \"\\u1EBF\")    # ế\n",
    "  text = text.replace(\"\\u00EA\\u0300\", \"\\u1EC1\")    # ề\n",
    "  text = text.replace(\"\\u00EA\\u0323\", \"\\u1EC7\")    # ệ\n",
    "  text = text.replace(\"\\u00EA\\u0303\", \"\\u1EC5\")    # ễ\n",
    "  text = text.replace(\"\\u0079\\u0309\", \"\\u1EF7\")    # ỷ\n",
    "  text = text.replace(\"\\u0079\\u0301\", \"\\u00FD\")    # ý\n",
    "  text = text.replace(\"\\u0079\\u0300\", \"\\u1EF3\")    # ỳ\n",
    "  text = text.replace(\"\\u0079\\u0323\", \"\\u1EF5\")    # ỵ\n",
    "  text = text.replace(\"\\u0079\\u0303\", \"\\u1EF9\")    # ỹ\n",
    "  text = text.replace(\"\\u0075\\u0309\", \"\\u1EE7\")    # ủ\n",
    "  text = text.replace(\"\\u0075\\u0301\", \"\\u00FA\")    # ú\n",
    "  text = text.replace(\"\\u0075\\u0300\", \"\\u00F9\")    # ù\n",
    "  text = text.replace(\"\\u0075\\u0323\", \"\\u1EE5\")    # ụ\n",
    "  text = text.replace(\"\\u0075\\u0303\", \"\\u0169\")    # ũ\n",
    "  text = text.replace(\"\\u01B0\\u0309\", \"\\u1EED\")    # ử\n",
    "  text = text.replace(\"\\u01B0\\u0301\", \"\\u1EE9\")    # ứ\n",
    "  text = text.replace(\"\\u01B0\\u0300\", \"\\u1EEB\")    # ừ\n",
    "  text = text.replace(\"\\u01B0\\u0323\", \"\\u1EF1\")    # ự\n",
    "  text = text.replace(\"\\u01B0\\u0303\", \"\\u1EEF\")    # ữ\n",
    "  text = text.replace(\"\\u0069\\u0309\", \"\\u1EC9\")    # ỉ\n",
    "  text = text.replace(\"\\u0069\\u0301\", \"\\u00ED\")    # í\n",
    "  text = text.replace(\"\\u0069\\u0300\", \"\\u00EC\")    # ì\n",
    "  text = text.replace(\"\\u0069\\u0323\", \"\\u1ECB\")    # ị\n",
    "  text = text.replace(\"\\u0069\\u0303\", \"\\u0129\")    # ĩ\n",
    "  text = text.replace(\"\\u006F\\u0309\", \"\\u1ECF\")    # ỏ\n",
    "  text = text.replace(\"\\u006F\\u0301\", \"\\u00F3\")    # ó\n",
    "  text = text.replace(\"\\u006F\\u0300\", \"\\u00F2\")    # ò\n",
    "  text = text.replace(\"\\u006F\\u0323\", \"\\u1ECD\")    # ọ\n",
    "  text = text.replace(\"\\u006F\\u0303\", \"\\u00F5\")    # õ\n",
    "  text = text.replace(\"\\u01A1\\u0309\", \"\\u1EDF\")    # ở\n",
    "  text = text.replace(\"\\u01A1\\u0301\", \"\\u1EDB\")    # ớ\n",
    "  text = text.replace(\"\\u01A1\\u0300\", \"\\u1EDD\")    # ờ\n",
    "  text = text.replace(\"\\u01A1\\u0323\", \"\\u1EE3\")    # ợ\n",
    "  text = text.replace(\"\\u01A1\\u0303\", \"\\u1EE1\")    # ỡ\n",
    "  text = text.replace(\"\\u00F4\\u0309\", \"\\u1ED5\")    # ổ\n",
    "  text = text.replace(\"\\u00F4\\u0301\", \"\\u1ED1\")    # ố\n",
    "  text = text.replace(\"\\u00F4\\u0300\", \"\\u1ED3\")    # ồ\n",
    "  text = text.replace(\"\\u00F4\\u0323\", \"\\u1ED9\")    # ộ\n",
    "  text = text.replace(\"\\u00F4\\u0303\", \"\\u1ED7\")    # ỗ\n",
    "  text = text.replace(\"\\u0061\\u0309\", \"\\u1EA3\")    # ả\n",
    "  text = text.replace(\"\\u0061\\u0301\", \"\\u00E1\")    # á\n",
    "  text = text.replace(\"\\u0061\\u0300\", \"\\u00E0\")    # à\n",
    "  text = text.replace(\"\\u0061\\u0323\", \"\\u1EA1\")    # ạ\n",
    "  text = text.replace(\"\\u0061\\u0303\", \"\\u00E3\")    # ã\n",
    "  text = text.replace(\"\\u0103\\u0309\", \"\\u1EB3\")    # ẳ\n",
    "  text = text.replace(\"\\u0103\\u0301\", \"\\u1EAF\")    # ắ\n",
    "  text = text.replace(\"\\u0103\\u0300\", \"\\u1EB1\")    # ằ\n",
    "  text = text.replace(\"\\u0103\\u0323\", \"\\u1EB7\")    # ặ\n",
    "  text = text.replace(\"\\u0103\\u0303\", \"\\u1EB5\")    # ẵ\n",
    "  text = text.replace(\"\\u00E2\\u0309\", \"\\u1EA9\")    # ẩ\n",
    "  text = text.replace(\"\\u00E2\\u0301\", \"\\u1EA5\")    # ấ\n",
    "  text = text.replace(\"\\u00E2\\u0300\", \"\\u1EA7\")    # ầ\n",
    "  text = text.replace(\"\\u00E2\\u0323\", \"\\u1EAD\")    # ậ\n",
    "  text = text.replace(\"\\u00E2\\u0303\", \"\\u1EAB\")    # ẫ\n",
    "  text = text.replace(\"\\u0045\\u0309\", \"\\u1EBA\")    # Ẻ\n",
    "  text = text.replace(\"\\u0045\\u0301\", \"\\u00C9\")    # É\n",
    "  text = text.replace(\"\\u0045\\u0300\", \"\\u00C8\")    # È\n",
    "  text = text.replace(\"\\u0045\\u0323\", \"\\u1EB8\")    # Ẹ\n",
    "  text = text.replace(\"\\u0045\\u0303\", \"\\u1EBC\")    # Ẽ\n",
    "  text = text.replace(\"\\u00CA\\u0309\", \"\\u1EC2\")    # Ể\n",
    "  text = text.replace(\"\\u00CA\\u0301\", \"\\u1EBE\")    # Ế\n",
    "  text = text.replace(\"\\u00CA\\u0300\", \"\\u1EC0\")    # Ề\n",
    "  text = text.replace(\"\\u00CA\\u0323\", \"\\u1EC6\")    # Ệ\n",
    "  text = text.replace(\"\\u00CA\\u0303\", \"\\u1EC4\")    # Ễ\n",
    "  text = text.replace(\"\\u0059\\u0309\", \"\\u1EF6\")    # Ỷ\n",
    "  text = text.replace(\"\\u0059\\u0301\", \"\\u00DD\")    # Ý\n",
    "  text = text.replace(\"\\u0059\\u0300\", \"\\u1EF2\")    # Ỳ\n",
    "  text = text.replace(\"\\u0059\\u0323\", \"\\u1EF4\")    # Ỵ\n",
    "  text = text.replace(\"\\u0059\\u0303\", \"\\u1EF8\")    # Ỹ\n",
    "  text = text.replace(\"\\u0055\\u0309\", \"\\u1EE6\")    # Ủ\n",
    "  text = text.replace(\"\\u0055\\u0301\", \"\\u00DA\")    # Ú\n",
    "  text = text.replace(\"\\u0055\\u0300\", \"\\u00D9\")    # Ù\n",
    "  text = text.replace(\"\\u0055\\u0323\", \"\\u1EE4\")    # Ụ\n",
    "  text = text.replace(\"\\u0055\\u0303\", \"\\u0168\")    # Ũ\n",
    "  text = text.replace(\"\\u01AF\\u0309\", \"\\u1EEC\")    # Ử\n",
    "  text = text.replace(\"\\u01AF\\u0301\", \"\\u1EE8\")    # Ứ\n",
    "  text = text.replace(\"\\u01AF\\u0300\", \"\\u1EEA\")    # Ừ\n",
    "  text = text.replace(\"\\u01AF\\u0323\", \"\\u1EF0\")    # Ự\n",
    "  text = text.replace(\"\\u01AF\\u0303\", \"\\u1EEE\")    # Ữ\n",
    "  text = text.replace(\"\\u0049\\u0309\", \"\\u1EC8\")    # Ỉ\n",
    "  text = text.replace(\"\\u0049\\u0301\", \"\\u00CD\")    # Í\n",
    "  text = text.replace(\"\\u0049\\u0300\", \"\\u00CC\")    # Ì\n",
    "  text = text.replace(\"\\u0049\\u0323\", \"\\u1ECA\")    # Ị\n",
    "  text = text.replace(\"\\u0049\\u0303\", \"\\u0128\")    # Ĩ\n",
    "  text = text.replace(\"\\u004F\\u0309\", \"\\u1ECE\")    # Ỏ\n",
    "  text = text.replace(\"\\u004F\\u0301\", \"\\u00D3\")    # Ó\n",
    "  text = text.replace(\"\\u004F\\u0300\", \"\\u00D2\")    # Ò\n",
    "  text = text.replace(\"\\u004F\\u0323\", \"\\u1ECC\")    # Ọ\n",
    "  text = text.replace(\"\\u004F\\u0303\", \"\\u00D5\")    # Õ\n",
    "  text = text.replace(\"\\u01A0\\u0309\", \"\\u1EDE\")    # Ở\n",
    "  text = text.replace(\"\\u01A0\\u0301\", \"\\u1EDA\")    # Ớ\n",
    "  text = text.replace(\"\\u01A0\\u0300\", \"\\u1EDC\")    # Ờ\n",
    "  text = text.replace(\"\\u01A0\\u0323\", \"\\u1EE2\")    # Ợ\n",
    "  text = text.replace(\"\\u01A0\\u0303\", \"\\u1EE0\")    # Ỡ\n",
    "  text = text.replace(\"\\u00D4\\u0309\", \"\\u1ED4\")    # Ổ\n",
    "  text = text.replace(\"\\u00D4\\u0301\", \"\\u1ED0\")    # Ố\n",
    "  text = text.replace(\"\\u00D4\\u0300\", \"\\u1ED2\")    # Ồ\n",
    "  text = text.replace(\"\\u00D4\\u0323\", \"\\u1ED8\")    # Ộ\n",
    "  text = text.replace(\"\\u00D4\\u0303\", \"\\u1ED6\")    # Ỗ\n",
    "  text = text.replace(\"\\u0041\\u0309\", \"\\u1EA2\")    # Ả\n",
    "  text = text.replace(\"\\u0041\\u0301\", \"\\u00C1\")    # Á\n",
    "  text = text.replace(\"\\u0041\\u0300\", \"\\u00C0\")    # À\n",
    "  text = text.replace(\"\\u0041\\u0323\", \"\\u1EA0\")    # Ạ\n",
    "  text = text.replace(\"\\u0041\\u0303\", \"\\u00C3\")    # Ã\n",
    "  text = text.replace(\"\\u0102\\u0309\", \"\\u1EB2\")    # Ẳ\n",
    "  text = text.replace(\"\\u0102\\u0301\", \"\\u1EAE\")    # Ắ\n",
    "  text = text.replace(\"\\u0102\\u0300\", \"\\u1EB0\")    # Ằ\n",
    "  text = text.replace(\"\\u0102\\u0323\", \"\\u1EB6\")    # Ặ\n",
    "  text = text.replace(\"\\u0102\\u0303\", \"\\u1EB4\")    # Ẵ\n",
    "  text = text.replace(\"\\u00C2\\u0309\", \"\\u1EA8\")    # Ẩ\n",
    "  text = text.replace(\"\\u00C2\\u0301\", \"\\u1EA4\")    # Ấ\n",
    "  text = text.replace(\"\\u00C2\\u0300\", \"\\u1EA6\")    # Ầ\n",
    "  text = text.replace(\"\\u00C2\\u0323\", \"\\u1EAC\")    # Ậ\n",
    "  text = text.replace(\"\\u00C2\\u0303\", \"\\u1EAA\")    # Ẫ\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_equation(sentence,list_entity):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence=normalized_sentence.split(' ')\n",
    "    list_result_entity=[]\n",
    "    list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "    for entity in list_normalized_entity:\n",
    "        list_token_entity=entity.split(' ')\n",
    "        for i in range(len(list_token_sentence)-len(list_token_entity)+1):\n",
    "            if list_token_entity==list_token_sentence[i:i+len(list_token_entity)]:\n",
    "                list_result_entity.append(entity)\n",
    "    return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"cho mình xin thông tin về mùa hè xanh của xuân tình nguyện đi bạn\"\n",
    "list_entity=[\"mùa hè xanh\",\"xuân tình nguyện\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mùa hè xanh', 'xuân tình nguyện']\n"
     ]
    }
   ],
   "source": [
    "print(find_entity_equation(sentence,list_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_sublist(a, b):\n",
    "    table = {}\n",
    "    l = 0\n",
    "    i_max = None\n",
    "    j_max = None\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            if ca == cb:\n",
    "                table[i, j] = table.get((i - 1, j - 1), 0) + 1\n",
    "                if table[i, j] > l:\n",
    "                    l = table[i, j]\n",
    "                    i_max=i\n",
    "                    j_max=j\n",
    "    if i_max != None:\n",
    "        return l,i_max - 1\n",
    "    return l,i_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_common_sublist(\"cho mình xin mùa hè xanh thông tin về mùa hè xanh cse của xuân tình nguyện đi bạn\".split(' '),'mùa hè xanh cse'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length(a, b):\n",
    "    table = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            table[i][j] = (\n",
    "                table[i - 1][j - 1] + 1 if ca == cb else\n",
    "                max(table[i][j - 1], table[i - 1][j]))\n",
    "#     print(table)\n",
    "    return table[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length(['t','t','b','t','c','c','t','t'],['u','u','b','t','c','u','u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix = [[0 for x in range(5)] for y in range(2)] \n",
    "Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length_ta(x, y):\n",
    "    m = len(x)\n",
    "    n = len(y)\n",
    "    c = [[0 for x in range(n+1)] for y in range(m+1)] \n",
    "    b = [['' for x in range(n+1)] for y in range(m+1)] \n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                c[i][j] = c[i-1][j-1] + 1\n",
    "                b[i][j] = '↖️'\n",
    "            elif c[i - 1][j] >= c[i][j-1]:\n",
    "                c[i][j] = c[i - 1][j] \n",
    "                b[i][j] = '⬆️'\n",
    "            else:\n",
    "                c[i][j] = c[i][j-1] \n",
    "                b[i][j] = '⬅️'\n",
    "    max_common_length = c[-1][-1]\n",
    "    i = m\n",
    "    j = n\n",
    "    pointer = c[i][j]\n",
    "    result_index = []\n",
    "    i_max = 0\n",
    "    max_length_in_sentence = 0\n",
    "    while pointer != 0:\n",
    "        if pointer == max_common_length and b[i][j]=='⬆️':\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='⬅️':\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬆️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬅️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "#     print(result_index)\n",
    "#     print(max_common_length)\n",
    "    if result_index != [] :\n",
    "        i_max = result_index[0]-1\n",
    "        max_length_in_sentence = result_index[0] -result_index[-1] +1\n",
    "    \n",
    "    return max_common_length,max_length_in_sentence,i_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9, 10)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length_ta(['t','t','b','t','c','c','t','t','t','t','c'],['u','u','b','t','c','c','u','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i,ca in enumerate(['z','a','b','f','c','d','t'],1):\n",
    "#     print(i)\n",
    "#     print(ca)\n",
    "'a' in 'ab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a: sentence\n",
    "# # b: entity\n",
    "# def longest_common_sublist_not_adjacent(a, b):\n",
    "#     max_common_length = 0\n",
    "#     i_max = 0\n",
    "#     result_a_slice = []\n",
    "#     max_length_in_sentence = 0\n",
    "#     n_gram_match = None\n",
    "#     #for loop qua n_gram\n",
    "#     for n_gram in range(1,len(a)):\n",
    "#         for i in range(0,len(a)-n_gram+1):\n",
    "#             if lcs_length(a[i:i + n_gram],b) > max_common_length and float(lcs_length(a[i:i + n_gram],b))/len(a[i:i + n_gram]) > 0.5:\n",
    "#                 max_length_in_sentence = len(a[i:i + n_gram])\n",
    "#                 n_gram_match = a[i:i + n_gram]\n",
    "# #                 print(n_gram_match)\n",
    "# #                 print(a[i:i+n_gram])\n",
    "# #                 print(max_common_length)\n",
    "#                 max_common_length = lcs_length(a[i:i+n_gram],b)\n",
    "#                 i_max = i + n_gram - 1\n",
    "# #     print(\"max_length: {0}\".format(max_length))\n",
    "# #     print(\"max_length_not_adjacent: {0}\".format(max))\n",
    "#     return max_common_length ,max_length_in_sentence, i_max  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_common_sublist(\"đi mùa hè xanh năm 2013 khoa điện tử là giải đáp câu hỏi phải không bạn\".split(' '),'năm 2013'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 7, 9)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length_ta('đi tử là giải đáp câu hỏi phải không bạn'.split(' '),'giải đáp các câu hỏi , thắc mắc về trường lớp của các bạn'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_longest_common(sentence,list_entity,entity_name):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence = normalized_sentence.split(' ')\n",
    "    list_result_entity = []\n",
    "    dict_max_len = {}\n",
    "    list_normalized_entity = [compound2unicode(entity) for entity in list_entity]\n",
    "    result = []\n",
    "    longest_common_length = None\n",
    "    end_common_index = None\n",
    "    for index, entity in enumerate(list_normalized_entity):\n",
    "#         print(entity)\n",
    "        list_token_entity = entity.split(' ')\n",
    "        if entity_name in ['register','reward','works','joiner']:\n",
    "            longest_common_length, max_length_in_sentence, end_common_index = lcs_length_ta(list_token_sentence,list_token_entity)\n",
    "        else:\n",
    "            longest_common_length, end_common_index = longest_common_sublist(list_token_sentence,list_token_entity)\n",
    "#         print(longest_common_length)\n",
    "        if longest_common_length!=0:\n",
    "            if entity_name in ['register','reward','works']:\n",
    "                longest_common_length_entity, max_length_in_sentence_entity, end_common_index_entity = lcs_length_ta(list_token_entity,list_token_sentence)\n",
    "                if float(longest_common_length)/max_length_in_sentence > 0.6 and float(longest_common_length_entity)/max_length_in_sentence_entity>0.6:\n",
    "                    dict_max_len[str(index)] = {'max_length_in_sentence':max_length_in_sentence,'longest_common_length':longest_common_length,'end_common_index':end_common_index}\n",
    "            else:\n",
    "                dict_max_len[str(index)] = {'longest_common_length':longest_common_length,'end_common_index':end_common_index}\n",
    "#         list_token_sentence = list_token_sentence[: end_common_index - longest_common_length] + list_token_sentence[end_common_index:]\n",
    "    max_longest_common_length=0\n",
    "#     print(dict_max_len)\n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']>max_longest_common_length:\n",
    "            max_longest_common_length=v['longest_common_length']\n",
    "    \n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']==max_longest_common_length:\n",
    "            if entity_name in ['register','reward','works']:\n",
    "                result.append({\"longest_common_entity_index\":int(k),\"longest_common_length\":v['max_length_in_sentence'],\"end_common_index\":v['end_common_index']})\n",
    "            else:\n",
    "                result.append({\"longest_common_entity_index\":int(k),\"longest_common_length\":v['longest_common_length'],\"end_common_index\":v['end_common_index']})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entity_longest_common(\"đi tử là giải đáp câu hỏi phải không bạn\",['giải đáp các câu hỏi , thắc mắc về trường lớp của các bạn'],'works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_entity_threshold(sentence,list_entity,threshold):\n",
    "#     normalized_sentence = compound2unicode(sentence)\n",
    "#     list_token_sentence = sentence.split(' ')\n",
    "#     dict_max_len = {}\n",
    "#     list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "#     for index, entity in enumerate(list_normalized_entity):\n",
    "#         list_token_entity = entity.split(' ')\n",
    "#         dict_max_len[str(index)] = longest_common_sublist(list_token_sentence,list_token_entity)        \n",
    "#     list_index_entity_threshold = [int(k) for k,v in dict_max_len.items() if v>=threshold]\n",
    "#     list_result_entity = [list_entity[index_entity_threshold] for index_entity_threshold in list_index_entity_threshold]\n",
    "#     return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_entity_threshold(\"cho mình xin thông tin mùa hè xanh khoa máy tính\",[\"mùa hè xanh\",\"mùa hè xanh khoa máy tính\",\"mùa hè xanh khoa\"],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/real_dict_915_shorted.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_intent_to_list_order_entity_name = {\n",
    "    'time':['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'name_activity':['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address'\\\n",
    "                            , 'reward'],\n",
    "    'type_activity':['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'holder':['holder','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'name_place':['name_place','time','name_activity', 'type_activity',  \\\n",
    "                            'holder', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'address':['address','time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place' , 'works'\\\n",
    "                            , 'reward'],\n",
    "    'contact':['contact', 'time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'works':['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address','works'\\\n",
    "                            , 'reward'],\n",
    "    'register':['register','time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'reward':['reward','time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address' ,'works'\\\n",
    "                            ],\n",
    "    'joiner':['joiner','time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'reward','works'\\\n",
    "                            ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(intent,input_sentence):\n",
    "    normalized_input_sentence=compound2unicode(input_sentence)\n",
    "    result_entity_dict={}\n",
    "    matching_threshold = 0.0\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    list_order_entity_name=['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'contact', 'works', 'register'\\\n",
    "                            , 'reward', 'joiner']\n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['works','register','reward']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in map_intent_to_list_order_entity_name[intent]:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        print(entity_name)\n",
    "        if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "            matching_threshold = 0.1\n",
    "        else:\n",
    "            matching_threshold = 0.6\n",
    "#         print(\"000. sentence:{0}\".format(normalized_input_sentence))\n",
    "        catch_entity_threshold_loop = 0\n",
    "        while True:\n",
    "            if catch_entity_threshold_loop > 5:\n",
    "                break\n",
    "            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)\n",
    "    #         print(list_dict_longest_common_entity)\n",
    "                #     [{'longest_common_entity_index': 0,\n",
    "                #   'longest_common_length': 3,\n",
    "                #   'end_common_index': 9}]\n",
    "\n",
    "            ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                                #/ length of entity )\n",
    "                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "            if list_dict_longest_common_entity == []:\n",
    "                break\n",
    "            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :\n",
    "                break\n",
    "            \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "#             print(\"list_sentence_token :{0}\".format(list_sentence_token))\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "#             print(\"common entity :{0}\".format(list_dict_longest_common_entity))\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                 print(\"0. dict_longest_common_entity: {0}\".format(dict_longest_common_entity))\n",
    "\n",
    "#                     print(\"duong\")\n",
    "#                 print(\"0.1 entity: {0}\".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]\n",
    "#                 print(\"2. list_sentence_token_match : {0}\".format(list_sentence_token_match))\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "#                 print(\"3. list_temp_longest_entity_token : {0}\".format(list_temp_longest_entity_token))\n",
    "                if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "#                     print(\"list_temp_longest_entity_token :{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"list_sentence_token_match :{0}\".format(list_sentence_token_match))\n",
    "                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)\n",
    "                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]\n",
    "                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))\n",
    "#                     print(\"list_entity_token_match: {0}\".format(list_entity_token_match))\n",
    "#                     print(\"list_temp_longest_entity_token:{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"score :{0}\".format(score))\n",
    "                else:\n",
    "                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                if score > max_match_entity:\n",
    "#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    max_match_entity = score\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "#             print(list_sentence_token)\n",
    "#             print(greatest_common_length)\n",
    "#             print(greatest_end_common_index)\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "#             print(\"1. greatest_common_length : {0}\".format(greatest_common_length))\n",
    "#             print(\"2. greatest entity : {0}\".format(list_entity[greatest_entity_index]))\n",
    "#             print(\"2.1 greatest_end_common_index: {0}\".format(greatest_end_common_index))\n",
    "#             print(\"3. sentence match: {0}\".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))\n",
    "            if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:\n",
    "                if entity_name in result_entity_dict:\n",
    "                    result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                else:\n",
    "                    result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "#                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]\n",
    "                list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = [\"✪\"]*greatest_common_length\n",
    "                normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "            catch_entity_threshold_loop = catch_entity_threshold_loop + 1\n",
    "            print(\"input sentence: {0}\".format(normalized_input_sentence))\n",
    "    return result_entity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/real_dict_v1_uncheck_shorted.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)\n",
    "    print(real_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward\n",
      "input sentence: đi mùa hè xanh ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "time\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "input sentence: đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ đúng không bạn\n",
      "name_activity\n",
      "type_activity\n",
      "holder\n",
      "name_place\n",
      "address\n",
      "works\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward': ['tích lũy được thêm ngày ctxh',\n",
       "  'thỏa mong muốn tham gia chiến dịch mùa hè xanh']}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_entity(\"reward\",\"đi mùa hè xanh được 12 ngày ctxh đúng không bạn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
