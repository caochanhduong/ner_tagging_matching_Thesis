{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound2unicode(text):\n",
    "  #https://gist.github.com/redphx/9320735`\n",
    "  text = text.replace(\"\\u0065\\u0309\", \"\\u1EBB\")    # ẻ\n",
    "  text = text.replace(\"\\u0065\\u0301\", \"\\u00E9\")    # é\n",
    "  text = text.replace(\"\\u0065\\u0300\", \"\\u00E8\")    # è\n",
    "  text = text.replace(\"\\u0065\\u0323\", \"\\u1EB9\")    # ẹ\n",
    "  text = text.replace(\"\\u0065\\u0303\", \"\\u1EBD\")    # ẽ\n",
    "  text = text.replace(\"\\u00EA\\u0309\", \"\\u1EC3\")    # ể\n",
    "  text = text.replace(\"\\u00EA\\u0301\", \"\\u1EBF\")    # ế\n",
    "  text = text.replace(\"\\u00EA\\u0300\", \"\\u1EC1\")    # ề\n",
    "  text = text.replace(\"\\u00EA\\u0323\", \"\\u1EC7\")    # ệ\n",
    "  text = text.replace(\"\\u00EA\\u0303\", \"\\u1EC5\")    # ễ\n",
    "  text = text.replace(\"\\u0079\\u0309\", \"\\u1EF7\")    # ỷ\n",
    "  text = text.replace(\"\\u0079\\u0301\", \"\\u00FD\")    # ý\n",
    "  text = text.replace(\"\\u0079\\u0300\", \"\\u1EF3\")    # ỳ\n",
    "  text = text.replace(\"\\u0079\\u0323\", \"\\u1EF5\")    # ỵ\n",
    "  text = text.replace(\"\\u0079\\u0303\", \"\\u1EF9\")    # ỹ\n",
    "  text = text.replace(\"\\u0075\\u0309\", \"\\u1EE7\")    # ủ\n",
    "  text = text.replace(\"\\u0075\\u0301\", \"\\u00FA\")    # ú\n",
    "  text = text.replace(\"\\u0075\\u0300\", \"\\u00F9\")    # ù\n",
    "  text = text.replace(\"\\u0075\\u0323\", \"\\u1EE5\")    # ụ\n",
    "  text = text.replace(\"\\u0075\\u0303\", \"\\u0169\")    # ũ\n",
    "  text = text.replace(\"\\u01B0\\u0309\", \"\\u1EED\")    # ử\n",
    "  text = text.replace(\"\\u01B0\\u0301\", \"\\u1EE9\")    # ứ\n",
    "  text = text.replace(\"\\u01B0\\u0300\", \"\\u1EEB\")    # ừ\n",
    "  text = text.replace(\"\\u01B0\\u0323\", \"\\u1EF1\")    # ự\n",
    "  text = text.replace(\"\\u01B0\\u0303\", \"\\u1EEF\")    # ữ\n",
    "  text = text.replace(\"\\u0069\\u0309\", \"\\u1EC9\")    # ỉ\n",
    "  text = text.replace(\"\\u0069\\u0301\", \"\\u00ED\")    # í\n",
    "  text = text.replace(\"\\u0069\\u0300\", \"\\u00EC\")    # ì\n",
    "  text = text.replace(\"\\u0069\\u0323\", \"\\u1ECB\")    # ị\n",
    "  text = text.replace(\"\\u0069\\u0303\", \"\\u0129\")    # ĩ\n",
    "  text = text.replace(\"\\u006F\\u0309\", \"\\u1ECF\")    # ỏ\n",
    "  text = text.replace(\"\\u006F\\u0301\", \"\\u00F3\")    # ó\n",
    "  text = text.replace(\"\\u006F\\u0300\", \"\\u00F2\")    # ò\n",
    "  text = text.replace(\"\\u006F\\u0323\", \"\\u1ECD\")    # ọ\n",
    "  text = text.replace(\"\\u006F\\u0303\", \"\\u00F5\")    # õ\n",
    "  text = text.replace(\"\\u01A1\\u0309\", \"\\u1EDF\")    # ở\n",
    "  text = text.replace(\"\\u01A1\\u0301\", \"\\u1EDB\")    # ớ\n",
    "  text = text.replace(\"\\u01A1\\u0300\", \"\\u1EDD\")    # ờ\n",
    "  text = text.replace(\"\\u01A1\\u0323\", \"\\u1EE3\")    # ợ\n",
    "  text = text.replace(\"\\u01A1\\u0303\", \"\\u1EE1\")    # ỡ\n",
    "  text = text.replace(\"\\u00F4\\u0309\", \"\\u1ED5\")    # ổ\n",
    "  text = text.replace(\"\\u00F4\\u0301\", \"\\u1ED1\")    # ố\n",
    "  text = text.replace(\"\\u00F4\\u0300\", \"\\u1ED3\")    # ồ\n",
    "  text = text.replace(\"\\u00F4\\u0323\", \"\\u1ED9\")    # ộ\n",
    "  text = text.replace(\"\\u00F4\\u0303\", \"\\u1ED7\")    # ỗ\n",
    "  text = text.replace(\"\\u0061\\u0309\", \"\\u1EA3\")    # ả\n",
    "  text = text.replace(\"\\u0061\\u0301\", \"\\u00E1\")    # á\n",
    "  text = text.replace(\"\\u0061\\u0300\", \"\\u00E0\")    # à\n",
    "  text = text.replace(\"\\u0061\\u0323\", \"\\u1EA1\")    # ạ\n",
    "  text = text.replace(\"\\u0061\\u0303\", \"\\u00E3\")    # ã\n",
    "  text = text.replace(\"\\u0103\\u0309\", \"\\u1EB3\")    # ẳ\n",
    "  text = text.replace(\"\\u0103\\u0301\", \"\\u1EAF\")    # ắ\n",
    "  text = text.replace(\"\\u0103\\u0300\", \"\\u1EB1\")    # ằ\n",
    "  text = text.replace(\"\\u0103\\u0323\", \"\\u1EB7\")    # ặ\n",
    "  text = text.replace(\"\\u0103\\u0303\", \"\\u1EB5\")    # ẵ\n",
    "  text = text.replace(\"\\u00E2\\u0309\", \"\\u1EA9\")    # ẩ\n",
    "  text = text.replace(\"\\u00E2\\u0301\", \"\\u1EA5\")    # ấ\n",
    "  text = text.replace(\"\\u00E2\\u0300\", \"\\u1EA7\")    # ầ\n",
    "  text = text.replace(\"\\u00E2\\u0323\", \"\\u1EAD\")    # ậ\n",
    "  text = text.replace(\"\\u00E2\\u0303\", \"\\u1EAB\")    # ẫ\n",
    "  text = text.replace(\"\\u0045\\u0309\", \"\\u1EBA\")    # Ẻ\n",
    "  text = text.replace(\"\\u0045\\u0301\", \"\\u00C9\")    # É\n",
    "  text = text.replace(\"\\u0045\\u0300\", \"\\u00C8\")    # È\n",
    "  text = text.replace(\"\\u0045\\u0323\", \"\\u1EB8\")    # Ẹ\n",
    "  text = text.replace(\"\\u0045\\u0303\", \"\\u1EBC\")    # Ẽ\n",
    "  text = text.replace(\"\\u00CA\\u0309\", \"\\u1EC2\")    # Ể\n",
    "  text = text.replace(\"\\u00CA\\u0301\", \"\\u1EBE\")    # Ế\n",
    "  text = text.replace(\"\\u00CA\\u0300\", \"\\u1EC0\")    # Ề\n",
    "  text = text.replace(\"\\u00CA\\u0323\", \"\\u1EC6\")    # Ệ\n",
    "  text = text.replace(\"\\u00CA\\u0303\", \"\\u1EC4\")    # Ễ\n",
    "  text = text.replace(\"\\u0059\\u0309\", \"\\u1EF6\")    # Ỷ\n",
    "  text = text.replace(\"\\u0059\\u0301\", \"\\u00DD\")    # Ý\n",
    "  text = text.replace(\"\\u0059\\u0300\", \"\\u1EF2\")    # Ỳ\n",
    "  text = text.replace(\"\\u0059\\u0323\", \"\\u1EF4\")    # Ỵ\n",
    "  text = text.replace(\"\\u0059\\u0303\", \"\\u1EF8\")    # Ỹ\n",
    "  text = text.replace(\"\\u0055\\u0309\", \"\\u1EE6\")    # Ủ\n",
    "  text = text.replace(\"\\u0055\\u0301\", \"\\u00DA\")    # Ú\n",
    "  text = text.replace(\"\\u0055\\u0300\", \"\\u00D9\")    # Ù\n",
    "  text = text.replace(\"\\u0055\\u0323\", \"\\u1EE4\")    # Ụ\n",
    "  text = text.replace(\"\\u0055\\u0303\", \"\\u0168\")    # Ũ\n",
    "  text = text.replace(\"\\u01AF\\u0309\", \"\\u1EEC\")    # Ử\n",
    "  text = text.replace(\"\\u01AF\\u0301\", \"\\u1EE8\")    # Ứ\n",
    "  text = text.replace(\"\\u01AF\\u0300\", \"\\u1EEA\")    # Ừ\n",
    "  text = text.replace(\"\\u01AF\\u0323\", \"\\u1EF0\")    # Ự\n",
    "  text = text.replace(\"\\u01AF\\u0303\", \"\\u1EEE\")    # Ữ\n",
    "  text = text.replace(\"\\u0049\\u0309\", \"\\u1EC8\")    # Ỉ\n",
    "  text = text.replace(\"\\u0049\\u0301\", \"\\u00CD\")    # Í\n",
    "  text = text.replace(\"\\u0049\\u0300\", \"\\u00CC\")    # Ì\n",
    "  text = text.replace(\"\\u0049\\u0323\", \"\\u1ECA\")    # Ị\n",
    "  text = text.replace(\"\\u0049\\u0303\", \"\\u0128\")    # Ĩ\n",
    "  text = text.replace(\"\\u004F\\u0309\", \"\\u1ECE\")    # Ỏ\n",
    "  text = text.replace(\"\\u004F\\u0301\", \"\\u00D3\")    # Ó\n",
    "  text = text.replace(\"\\u004F\\u0300\", \"\\u00D2\")    # Ò\n",
    "  text = text.replace(\"\\u004F\\u0323\", \"\\u1ECC\")    # Ọ\n",
    "  text = text.replace(\"\\u004F\\u0303\", \"\\u00D5\")    # Õ\n",
    "  text = text.replace(\"\\u01A0\\u0309\", \"\\u1EDE\")    # Ở\n",
    "  text = text.replace(\"\\u01A0\\u0301\", \"\\u1EDA\")    # Ớ\n",
    "  text = text.replace(\"\\u01A0\\u0300\", \"\\u1EDC\")    # Ờ\n",
    "  text = text.replace(\"\\u01A0\\u0323\", \"\\u1EE2\")    # Ợ\n",
    "  text = text.replace(\"\\u01A0\\u0303\", \"\\u1EE0\")    # Ỡ\n",
    "  text = text.replace(\"\\u00D4\\u0309\", \"\\u1ED4\")    # Ổ\n",
    "  text = text.replace(\"\\u00D4\\u0301\", \"\\u1ED0\")    # Ố\n",
    "  text = text.replace(\"\\u00D4\\u0300\", \"\\u1ED2\")    # Ồ\n",
    "  text = text.replace(\"\\u00D4\\u0323\", \"\\u1ED8\")    # Ộ\n",
    "  text = text.replace(\"\\u00D4\\u0303\", \"\\u1ED6\")    # Ỗ\n",
    "  text = text.replace(\"\\u0041\\u0309\", \"\\u1EA2\")    # Ả\n",
    "  text = text.replace(\"\\u0041\\u0301\", \"\\u00C1\")    # Á\n",
    "  text = text.replace(\"\\u0041\\u0300\", \"\\u00C0\")    # À\n",
    "  text = text.replace(\"\\u0041\\u0323\", \"\\u1EA0\")    # Ạ\n",
    "  text = text.replace(\"\\u0041\\u0303\", \"\\u00C3\")    # Ã\n",
    "  text = text.replace(\"\\u0102\\u0309\", \"\\u1EB2\")    # Ẳ\n",
    "  text = text.replace(\"\\u0102\\u0301\", \"\\u1EAE\")    # Ắ\n",
    "  text = text.replace(\"\\u0102\\u0300\", \"\\u1EB0\")    # Ằ\n",
    "  text = text.replace(\"\\u0102\\u0323\", \"\\u1EB6\")    # Ặ\n",
    "  text = text.replace(\"\\u0102\\u0303\", \"\\u1EB4\")    # Ẵ\n",
    "  text = text.replace(\"\\u00C2\\u0309\", \"\\u1EA8\")    # Ẩ\n",
    "  text = text.replace(\"\\u00C2\\u0301\", \"\\u1EA4\")    # Ấ\n",
    "  text = text.replace(\"\\u00C2\\u0300\", \"\\u1EA6\")    # Ầ\n",
    "  text = text.replace(\"\\u00C2\\u0323\", \"\\u1EAC\")    # Ậ\n",
    "  text = text.replace(\"\\u00C2\\u0303\", \"\\u1EAA\")    # Ẫ\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_equation(sentence,list_entity):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence=normalized_sentence.split(' ')\n",
    "    list_result_entity=[]\n",
    "    list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "    for entity in list_normalized_entity:\n",
    "        list_token_entity=entity.split(' ')\n",
    "        for i in range(len(list_token_sentence)-len(list_token_entity)+1):\n",
    "            if list_token_entity==list_token_sentence[i:i+len(list_token_entity)]:\n",
    "                list_result_entity.append(entity)\n",
    "    return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"cho mình xin thông tin về mùa hè xanh của xuân tình nguyện đi bạn\"\n",
    "list_entity=[\"mùa hè xanh\",\"xuân tình nguyện\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mùa hè xanh', 'xuân tình nguyện']\n"
     ]
    }
   ],
   "source": [
    "print(find_entity_equation(sentence,list_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_sublist(a, b):\n",
    "    table = {}\n",
    "    l = 0\n",
    "    i_max = None\n",
    "    j_max = None\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            if ca == cb:\n",
    "                table[i, j] = table.get((i - 1, j - 1), 0) + 1\n",
    "                if table[i, j] > l:\n",
    "                    l = table[i, j]\n",
    "                    i_max=i\n",
    "                    j_max=j\n",
    "    return l,i_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_common_sublist(\"cho mình xin mùa hè xanh thông tin về mùa hè xanh cse của xuân tình nguyện đi bạn\".split(' '),'mùa hè xanh cse'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_longest_common(sentence,list_entity):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence = normalized_sentence.split(' ')\n",
    "    list_result_entity = []\n",
    "    dict_max_len = {}\n",
    "    list_normalized_entity = [compound2unicode(entity) for entity in list_entity]\n",
    "    result = []\n",
    "    longest_common_length = None\n",
    "    end_common_index = None\n",
    "    for index, entity in enumerate(list_normalized_entity):\n",
    "#         print(entity)\n",
    "        list_token_entity = entity.split(' ')\n",
    "        longest_common_length, end_common_index = longest_common_sublist(list_token_sentence,list_token_entity)\n",
    "#         print(longest_common_length)\n",
    "        if longest_common_length!=0:\n",
    "            dict_max_len[str(index)] = {'longest_common_length':longest_common_length,'end_common_index':end_common_index}\n",
    "#         list_token_sentence = list_token_sentence[: end_common_index - longest_common_length] + list_token_sentence[end_common_index:]\n",
    "    max_longest_common_length=0\n",
    "#     print(dict_max_len)\n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']>max_longest_common_length:\n",
    "            max_longest_common_length=v['longest_common_length']\n",
    "    \n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']==max_longest_common_length:\n",
    "            result.append({\"longest_common_entity_index\":int(k),\"longest_common_length\":v['longest_common_length'],\"end_common_index\":v['end_common_index']})\n",
    "         \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'longest_common_entity_index': 0,\n",
       "  'longest_common_length': 3,\n",
       "  'end_common_index': 9},\n",
       " {'longest_common_entity_index': 1,\n",
       "  'longest_common_length': 3,\n",
       "  'end_common_index': 9}]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entity_longest_common(\"cho mình xin thông tin về mùa hè xanh khoa máy tính\",['mùa hè xanh','mùa hè xanh năm 2013'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_entity_threshold(sentence,list_entity,threshold):\n",
    "#     normalized_sentence = compound2unicode(sentence)\n",
    "#     list_token_sentence = sentence.split(' ')\n",
    "#     dict_max_len = {}\n",
    "#     list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "#     for index, entity in enumerate(list_normalized_entity):\n",
    "#         list_token_entity = entity.split(' ')\n",
    "#         dict_max_len[str(index)] = longest_common_sublist(list_token_sentence,list_token_entity)        \n",
    "#     list_index_entity_threshold = [int(k) for k,v in dict_max_len.items() if v>=threshold]\n",
    "#     list_result_entity = [list_entity[index_entity_threshold] for index_entity_threshold in list_index_entity_threshold]\n",
    "#     return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_entity_threshold(\"cho mình xin thông tin mùa hè xanh khoa máy tính\",[\"mùa hè xanh\",\"mùa hè xanh khoa máy tính\",\"mùa hè xanh khoa\"],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/real_dict_915_shorted.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(input_sentence):\n",
    "    normalized_input_sentence=compound2unicode(input_sentence)\n",
    "    result_entity_dict={}\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    list_order_entity_name=['time','name_activity', 'type_activity', 'holder', \\\n",
    "                            'name_place', 'address', 'contact', 'reward', 'register'\\\n",
    "                            , 'works', 'joiner']\n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['reward','register','works']:\n",
    "            map_entity_name_to_threshold[entity_name]=4\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in list_order_entity_name:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        print(entity_name)\n",
    "        list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity)\n",
    "            #     [{'longest_common_entity_index': 0,\n",
    "            #   'longest_common_length': 3,\n",
    "            #   'end_common_index': 9}]\n",
    "\n",
    "        ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                            #/ length of entity )\n",
    "                        ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "        if list_dict_longest_common_entity != []:    \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                     print(\"duong\")\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length:end_common_index]\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "                if len(list_sentence_token_match)/float(len(list_temp_longest_entity_token)) > max_match_entity:\n",
    "                    max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "\n",
    "\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "            if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > 0.6:\n",
    "                result_entity_dict[entity_name] = list_entity[greatest_entity_index]\n",
    "                list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length] + list_sentence_token[greatest_end_common_index:]\n",
    "                normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "    return result_entity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/real_dict_915_shorted.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)\n",
    "    print(real_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "name_activity\n",
      "type_activity\n",
      "holder\n",
      "name_place\n",
      "address\n",
      "contact\n",
      "reward\n",
      "register\n",
      "works\n",
      "joiner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name_activity': 'mùa hè xanh', 'holder': 'khoa điện'}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_entity(\"cho mình xin thông tin về mùa hè xanh của khoa điện tử\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
