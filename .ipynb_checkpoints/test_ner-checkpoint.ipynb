{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound2unicode(text):\n",
    "  #https://gist.github.com/redphx/9320735`\n",
    "  text = text.replace(\"\\u0065\\u0309\", \"\\u1EBB\")    # ẻ\n",
    "  text = text.replace(\"\\u0065\\u0301\", \"\\u00E9\")    # é\n",
    "  text = text.replace(\"\\u0065\\u0300\", \"\\u00E8\")    # è\n",
    "  text = text.replace(\"\\u0065\\u0323\", \"\\u1EB9\")    # ẹ\n",
    "  text = text.replace(\"\\u0065\\u0303\", \"\\u1EBD\")    # ẽ\n",
    "  text = text.replace(\"\\u00EA\\u0309\", \"\\u1EC3\")    # ể\n",
    "  text = text.replace(\"\\u00EA\\u0301\", \"\\u1EBF\")    # ế\n",
    "  text = text.replace(\"\\u00EA\\u0300\", \"\\u1EC1\")    # ề\n",
    "  text = text.replace(\"\\u00EA\\u0323\", \"\\u1EC7\")    # ệ\n",
    "  text = text.replace(\"\\u00EA\\u0303\", \"\\u1EC5\")    # ễ\n",
    "  text = text.replace(\"\\u0079\\u0309\", \"\\u1EF7\")    # ỷ\n",
    "  text = text.replace(\"\\u0079\\u0301\", \"\\u00FD\")    # ý\n",
    "  text = text.replace(\"\\u0079\\u0300\", \"\\u1EF3\")    # ỳ\n",
    "  text = text.replace(\"\\u0079\\u0323\", \"\\u1EF5\")    # ỵ\n",
    "  text = text.replace(\"\\u0079\\u0303\", \"\\u1EF9\")    # ỹ\n",
    "  text = text.replace(\"\\u0075\\u0309\", \"\\u1EE7\")    # ủ\n",
    "  text = text.replace(\"\\u0075\\u0301\", \"\\u00FA\")    # ú\n",
    "  text = text.replace(\"\\u0075\\u0300\", \"\\u00F9\")    # ù\n",
    "  text = text.replace(\"\\u0075\\u0323\", \"\\u1EE5\")    # ụ\n",
    "  text = text.replace(\"\\u0075\\u0303\", \"\\u0169\")    # ũ\n",
    "  text = text.replace(\"\\u01B0\\u0309\", \"\\u1EED\")    # ử\n",
    "  text = text.replace(\"\\u01B0\\u0301\", \"\\u1EE9\")    # ứ\n",
    "  text = text.replace(\"\\u01B0\\u0300\", \"\\u1EEB\")    # ừ\n",
    "  text = text.replace(\"\\u01B0\\u0323\", \"\\u1EF1\")    # ự\n",
    "  text = text.replace(\"\\u01B0\\u0303\", \"\\u1EEF\")    # ữ\n",
    "  text = text.replace(\"\\u0069\\u0309\", \"\\u1EC9\")    # ỉ\n",
    "  text = text.replace(\"\\u0069\\u0301\", \"\\u00ED\")    # í\n",
    "  text = text.replace(\"\\u0069\\u0300\", \"\\u00EC\")    # ì\n",
    "  text = text.replace(\"\\u0069\\u0323\", \"\\u1ECB\")    # ị\n",
    "  text = text.replace(\"\\u0069\\u0303\", \"\\u0129\")    # ĩ\n",
    "  text = text.replace(\"\\u006F\\u0309\", \"\\u1ECF\")    # ỏ\n",
    "  text = text.replace(\"\\u006F\\u0301\", \"\\u00F3\")    # ó\n",
    "  text = text.replace(\"\\u006F\\u0300\", \"\\u00F2\")    # ò\n",
    "  text = text.replace(\"\\u006F\\u0323\", \"\\u1ECD\")    # ọ\n",
    "  text = text.replace(\"\\u006F\\u0303\", \"\\u00F5\")    # õ\n",
    "  text = text.replace(\"\\u01A1\\u0309\", \"\\u1EDF\")    # ở\n",
    "  text = text.replace(\"\\u01A1\\u0301\", \"\\u1EDB\")    # ớ\n",
    "  text = text.replace(\"\\u01A1\\u0300\", \"\\u1EDD\")    # ờ\n",
    "  text = text.replace(\"\\u01A1\\u0323\", \"\\u1EE3\")    # ợ\n",
    "  text = text.replace(\"\\u01A1\\u0303\", \"\\u1EE1\")    # ỡ\n",
    "  text = text.replace(\"\\u00F4\\u0309\", \"\\u1ED5\")    # ổ\n",
    "  text = text.replace(\"\\u00F4\\u0301\", \"\\u1ED1\")    # ố\n",
    "  text = text.replace(\"\\u00F4\\u0300\", \"\\u1ED3\")    # ồ\n",
    "  text = text.replace(\"\\u00F4\\u0323\", \"\\u1ED9\")    # ộ\n",
    "  text = text.replace(\"\\u00F4\\u0303\", \"\\u1ED7\")    # ỗ\n",
    "  text = text.replace(\"\\u0061\\u0309\", \"\\u1EA3\")    # ả\n",
    "  text = text.replace(\"\\u0061\\u0301\", \"\\u00E1\")    # á\n",
    "  text = text.replace(\"\\u0061\\u0300\", \"\\u00E0\")    # à\n",
    "  text = text.replace(\"\\u0061\\u0323\", \"\\u1EA1\")    # ạ\n",
    "  text = text.replace(\"\\u0061\\u0303\", \"\\u00E3\")    # ã\n",
    "  text = text.replace(\"\\u0103\\u0309\", \"\\u1EB3\")    # ẳ\n",
    "  text = text.replace(\"\\u0103\\u0301\", \"\\u1EAF\")    # ắ\n",
    "  text = text.replace(\"\\u0103\\u0300\", \"\\u1EB1\")    # ằ\n",
    "  text = text.replace(\"\\u0103\\u0323\", \"\\u1EB7\")    # ặ\n",
    "  text = text.replace(\"\\u0103\\u0303\", \"\\u1EB5\")    # ẵ\n",
    "  text = text.replace(\"\\u00E2\\u0309\", \"\\u1EA9\")    # ẩ\n",
    "  text = text.replace(\"\\u00E2\\u0301\", \"\\u1EA5\")    # ấ\n",
    "  text = text.replace(\"\\u00E2\\u0300\", \"\\u1EA7\")    # ầ\n",
    "  text = text.replace(\"\\u00E2\\u0323\", \"\\u1EAD\")    # ậ\n",
    "  text = text.replace(\"\\u00E2\\u0303\", \"\\u1EAB\")    # ẫ\n",
    "  text = text.replace(\"\\u0045\\u0309\", \"\\u1EBA\")    # Ẻ\n",
    "  text = text.replace(\"\\u0045\\u0301\", \"\\u00C9\")    # É\n",
    "  text = text.replace(\"\\u0045\\u0300\", \"\\u00C8\")    # È\n",
    "  text = text.replace(\"\\u0045\\u0323\", \"\\u1EB8\")    # Ẹ\n",
    "  text = text.replace(\"\\u0045\\u0303\", \"\\u1EBC\")    # Ẽ\n",
    "  text = text.replace(\"\\u00CA\\u0309\", \"\\u1EC2\")    # Ể\n",
    "  text = text.replace(\"\\u00CA\\u0301\", \"\\u1EBE\")    # Ế\n",
    "  text = text.replace(\"\\u00CA\\u0300\", \"\\u1EC0\")    # Ề\n",
    "  text = text.replace(\"\\u00CA\\u0323\", \"\\u1EC6\")    # Ệ\n",
    "  text = text.replace(\"\\u00CA\\u0303\", \"\\u1EC4\")    # Ễ\n",
    "  text = text.replace(\"\\u0059\\u0309\", \"\\u1EF6\")    # Ỷ\n",
    "  text = text.replace(\"\\u0059\\u0301\", \"\\u00DD\")    # Ý\n",
    "  text = text.replace(\"\\u0059\\u0300\", \"\\u1EF2\")    # Ỳ\n",
    "  text = text.replace(\"\\u0059\\u0323\", \"\\u1EF4\")    # Ỵ\n",
    "  text = text.replace(\"\\u0059\\u0303\", \"\\u1EF8\")    # Ỹ\n",
    "  text = text.replace(\"\\u0055\\u0309\", \"\\u1EE6\")    # Ủ\n",
    "  text = text.replace(\"\\u0055\\u0301\", \"\\u00DA\")    # Ú\n",
    "  text = text.replace(\"\\u0055\\u0300\", \"\\u00D9\")    # Ù\n",
    "  text = text.replace(\"\\u0055\\u0323\", \"\\u1EE4\")    # Ụ\n",
    "  text = text.replace(\"\\u0055\\u0303\", \"\\u0168\")    # Ũ\n",
    "  text = text.replace(\"\\u01AF\\u0309\", \"\\u1EEC\")    # Ử\n",
    "  text = text.replace(\"\\u01AF\\u0301\", \"\\u1EE8\")    # Ứ\n",
    "  text = text.replace(\"\\u01AF\\u0300\", \"\\u1EEA\")    # Ừ\n",
    "  text = text.replace(\"\\u01AF\\u0323\", \"\\u1EF0\")    # Ự\n",
    "  text = text.replace(\"\\u01AF\\u0303\", \"\\u1EEE\")    # Ữ\n",
    "  text = text.replace(\"\\u0049\\u0309\", \"\\u1EC8\")    # Ỉ\n",
    "  text = text.replace(\"\\u0049\\u0301\", \"\\u00CD\")    # Í\n",
    "  text = text.replace(\"\\u0049\\u0300\", \"\\u00CC\")    # Ì\n",
    "  text = text.replace(\"\\u0049\\u0323\", \"\\u1ECA\")    # Ị\n",
    "  text = text.replace(\"\\u0049\\u0303\", \"\\u0128\")    # Ĩ\n",
    "  text = text.replace(\"\\u004F\\u0309\", \"\\u1ECE\")    # Ỏ\n",
    "  text = text.replace(\"\\u004F\\u0301\", \"\\u00D3\")    # Ó\n",
    "  text = text.replace(\"\\u004F\\u0300\", \"\\u00D2\")    # Ò\n",
    "  text = text.replace(\"\\u004F\\u0323\", \"\\u1ECC\")    # Ọ\n",
    "  text = text.replace(\"\\u004F\\u0303\", \"\\u00D5\")    # Õ\n",
    "  text = text.replace(\"\\u01A0\\u0309\", \"\\u1EDE\")    # Ở\n",
    "  text = text.replace(\"\\u01A0\\u0301\", \"\\u1EDA\")    # Ớ\n",
    "  text = text.replace(\"\\u01A0\\u0300\", \"\\u1EDC\")    # Ờ\n",
    "  text = text.replace(\"\\u01A0\\u0323\", \"\\u1EE2\")    # Ợ\n",
    "  text = text.replace(\"\\u01A0\\u0303\", \"\\u1EE0\")    # Ỡ\n",
    "  text = text.replace(\"\\u00D4\\u0309\", \"\\u1ED4\")    # Ổ\n",
    "  text = text.replace(\"\\u00D4\\u0301\", \"\\u1ED0\")    # Ố\n",
    "  text = text.replace(\"\\u00D4\\u0300\", \"\\u1ED2\")    # Ồ\n",
    "  text = text.replace(\"\\u00D4\\u0323\", \"\\u1ED8\")    # Ộ\n",
    "  text = text.replace(\"\\u00D4\\u0303\", \"\\u1ED6\")    # Ỗ\n",
    "  text = text.replace(\"\\u0041\\u0309\", \"\\u1EA2\")    # Ả\n",
    "  text = text.replace(\"\\u0041\\u0301\", \"\\u00C1\")    # Á\n",
    "  text = text.replace(\"\\u0041\\u0300\", \"\\u00C0\")    # À\n",
    "  text = text.replace(\"\\u0041\\u0323\", \"\\u1EA0\")    # Ạ\n",
    "  text = text.replace(\"\\u0041\\u0303\", \"\\u00C3\")    # Ã\n",
    "  text = text.replace(\"\\u0102\\u0309\", \"\\u1EB2\")    # Ẳ\n",
    "  text = text.replace(\"\\u0102\\u0301\", \"\\u1EAE\")    # Ắ\n",
    "  text = text.replace(\"\\u0102\\u0300\", \"\\u1EB0\")    # Ằ\n",
    "  text = text.replace(\"\\u0102\\u0323\", \"\\u1EB6\")    # Ặ\n",
    "  text = text.replace(\"\\u0102\\u0303\", \"\\u1EB4\")    # Ẵ\n",
    "  text = text.replace(\"\\u00C2\\u0309\", \"\\u1EA8\")    # Ẩ\n",
    "  text = text.replace(\"\\u00C2\\u0301\", \"\\u1EA4\")    # Ấ\n",
    "  text = text.replace(\"\\u00C2\\u0300\", \"\\u1EA6\")    # Ầ\n",
    "  text = text.replace(\"\\u00C2\\u0323\", \"\\u1EAC\")    # Ậ\n",
    "  text = text.replace(\"\\u00C2\\u0303\", \"\\u1EAA\")    # Ẫ\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_equation(sentence,list_entity):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence=normalized_sentence.split(' ')\n",
    "    list_result_entity=[]\n",
    "    list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "    for entity in list_normalized_entity:\n",
    "        list_token_entity=entity.split(' ')\n",
    "        for i in range(len(list_token_sentence)-len(list_token_entity)+1):\n",
    "            if list_token_entity==list_token_sentence[i:i+len(list_token_entity)]:\n",
    "                list_result_entity.append(entity)\n",
    "    return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"cho mình xin thông tin về mùa hè xanh của xuân tình nguyện đi bạn\"\n",
    "list_entity=[\"mùa hè xanh\",\"xuân tình nguyện\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mùa hè xanh', 'xuân tình nguyện']\n"
     ]
    }
   ],
   "source": [
    "print(find_entity_equation(sentence,list_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_sublist(a, b):\n",
    "    table = {}\n",
    "    l = 0\n",
    "    i_max = None\n",
    "    j_max = None\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            if ca == cb:\n",
    "                table[i, j] = table.get((i - 1, j - 1), 0) + 1\n",
    "                if table[i, j] > l:\n",
    "                    l = table[i, j]\n",
    "                    i_max=i\n",
    "                    j_max=j\n",
    "    if i_max != None:\n",
    "        return l,i_max - 1\n",
    "    return l,i_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_common_sublist(\"cho mình xin mùa hè xanh thông tin về mùa hè xanh cse của xuân tình nguyện đi bạn\".split(' '),'mùa hè xanh cse'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length(a, b):\n",
    "    table = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            table[i][j] = (\n",
    "                table[i - 1][j - 1] + 1 if ca == cb else\n",
    "                max(table[i][j - 1], table[i - 1][j]))\n",
    "#     print(table)\n",
    "    return table[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length(['t','t','b','t','c','c','t','t'],['u','u','b','t','c','u','u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix = [[0 for x in range(5)] for y in range(2)] \n",
    "Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length_ta(x, y):\n",
    "    m = len(x)\n",
    "    n = len(y)\n",
    "    c = [[0 for x in range(n+1)] for y in range(m+1)] \n",
    "    b = [['' for x in range(n+1)] for y in range(m+1)] \n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                c[i][j] = c[i-1][j-1] + 1\n",
    "                b[i][j] = '↖️'\n",
    "            elif c[i - 1][j] >= c[i][j-1]:\n",
    "                c[i][j] = c[i - 1][j] \n",
    "                b[i][j] = '⬆️'\n",
    "            else:\n",
    "                c[i][j] = c[i][j-1] \n",
    "                b[i][j] = '⬅️'\n",
    "    max_common_length = c[-1][-1]\n",
    "    i = m\n",
    "    j = n\n",
    "    pointer = c[i][j]\n",
    "    result_index = []\n",
    "    i_max = 0\n",
    "    max_length_in_sentence = 0\n",
    "    while pointer != 0:\n",
    "        if pointer == max_common_length and b[i][j]=='⬆️':\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='⬅️':\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer == max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬆️':\n",
    "            result_index.append(i)\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='⬅️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            pointer = c[i][j]\n",
    "        if pointer != max_common_length and b[i][j]=='↖️':\n",
    "            result_index.append(i)\n",
    "            j = j-1\n",
    "            i = i-1\n",
    "            pointer = c[i][j]\n",
    "#     print(result_index)\n",
    "#     print(max_common_length)\n",
    "    if result_index != [] :\n",
    "        i_max = result_index[0]-1\n",
    "        max_length_in_sentence = result_index[0] -result_index[-1] +1\n",
    "    \n",
    "    return max_common_length,max_length_in_sentence,i_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length_ta(['t','t','b','t','c','c','t','t','t','t','c'],['u','u','b','t','c','c','u','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a: sentence\n",
    "# # b: entity\n",
    "# def longest_common_sublist_not_adjacent(a, b):\n",
    "#     max_common_length = 0\n",
    "#     i_max = 0\n",
    "#     result_a_slice = []\n",
    "#     max_length_in_sentence = 0\n",
    "#     n_gram_match = None\n",
    "#     #for loop qua n_gram\n",
    "#     for n_gram in range(1,len(a)):\n",
    "#         for i in range(0,len(a)-n_gram+1):\n",
    "#             if lcs_length(a[i:i + n_gram],b) > max_common_length and float(lcs_length(a[i:i + n_gram],b))/len(a[i:i + n_gram]) > 0.5:\n",
    "#                 max_length_in_sentence = len(a[i:i + n_gram])\n",
    "#                 n_gram_match = a[i:i + n_gram]\n",
    "# #                 print(n_gram_match)\n",
    "# #                 print(a[i:i+n_gram])\n",
    "# #                 print(max_common_length)\n",
    "#                 max_common_length = lcs_length(a[i:i+n_gram],b)\n",
    "#                 i_max = i + n_gram - 1\n",
    "# #     print(\"max_length: {0}\".format(max_length))\n",
    "# #     print(\"max_length_not_adjacent: {0}\".format(max))\n",
    "#     return max_common_length ,max_length_in_sentence, i_max  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_common_sublist(\"đi mùa hè xanh năm 2013 khoa điện tử là giải đáp câu hỏi phải không bạn\".split(' '),'năm 2013'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 7, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length_ta('đi tử là giải đáp câu hỏi phải không bạn'.split(' '),'giải đáp các câu hỏi , thắc mắc về trường lớp của các bạn'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_longest_common(sentence,list_entity,entity_name):\n",
    "    normalized_sentence=compound2unicode(sentence)\n",
    "    list_token_sentence = normalized_sentence.split(' ')\n",
    "    list_result_entity = []\n",
    "    dict_max_len = {}\n",
    "    list_normalized_entity = [compound2unicode(entity) for entity in list_entity]\n",
    "    result = []\n",
    "    longest_common_length = None\n",
    "    end_common_index = None\n",
    "    for index, entity in enumerate(list_normalized_entity):\n",
    "#         print(entity)\n",
    "        list_token_entity = entity.split(' ')\n",
    "        if entity_name in ['register','reward','works','joiner']:\n",
    "            longest_common_length, max_length_in_sentence, end_common_index = lcs_length_ta(list_token_sentence,list_token_entity)\n",
    "        else:\n",
    "            longest_common_length, end_common_index = longest_common_sublist(list_token_sentence,list_token_entity)\n",
    "#         print(longest_common_length)\n",
    "        if longest_common_length!=0:\n",
    "            if entity_name in ['register','reward','works']:\n",
    "                longest_common_length_entity, max_length_in_sentence_entity, end_common_index_entity = lcs_length_ta(list_token_entity,list_token_sentence)\n",
    "                if float(longest_common_length)/max_length_in_sentence > 0.6 and float(longest_common_length_entity)/max_length_in_sentence_entity>0.6:\n",
    "                    dict_max_len[str(index)] = {'max_length_in_sentence':max_length_in_sentence,'longest_common_length':longest_common_length,'end_common_index':end_common_index}\n",
    "            else:\n",
    "                dict_max_len[str(index)] = {'longest_common_length':longest_common_length,'end_common_index':end_common_index}\n",
    "#         list_token_sentence = list_token_sentence[: end_common_index - longest_common_length] + list_token_sentence[end_common_index:]\n",
    "    max_longest_common_length=0\n",
    "#     print(dict_max_len)\n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']>max_longest_common_length:\n",
    "            max_longest_common_length=v['longest_common_length']\n",
    "    \n",
    "    for k,v in dict_max_len.items():\n",
    "        if v['longest_common_length']==max_longest_common_length:\n",
    "            if entity_name in ['register','reward','works']:\n",
    "                result.append({\"longest_common_entity_index\":int(k),\"longest_common_length\":v['max_length_in_sentence'],\"end_common_index\":v['end_common_index']})\n",
    "            else:\n",
    "                result.append({\"longest_common_entity_index\":int(k),\"longest_common_length\":v['longest_common_length'],\"end_common_index\":v['end_common_index']})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entity_longest_common(\"đi tử là giải đáp câu hỏi phải không bạn\",['giải đáp các câu hỏi , thắc mắc về trường lớp của các bạn'],'works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_entity_threshold(sentence,list_entity,threshold):\n",
    "#     normalized_sentence = compound2unicode(sentence)\n",
    "#     list_token_sentence = sentence.split(' ')\n",
    "#     dict_max_len = {}\n",
    "#     list_normalized_entity=[compound2unicode(entity) for entity in list_entity]\n",
    "#     for index, entity in enumerate(list_normalized_entity):\n",
    "#         list_token_entity = entity.split(' ')\n",
    "#         dict_max_len[str(index)] = longest_common_sublist(list_token_sentence,list_token_entity)        \n",
    "#     list_index_entity_threshold = [int(k) for k,v in dict_max_len.items() if v>=threshold]\n",
    "#     list_result_entity = [list_entity[index_entity_threshold] for index_entity_threshold in list_index_entity_threshold]\n",
    "#     return list_result_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_entity_threshold(\"cho mình xin thông tin mùa hè xanh khoa máy tính\",[\"mùa hè xanh\",\"mùa hè xanh khoa máy tính\",\"mùa hè xanh khoa\"],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/real_dict_2000.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_intent_to_list_order_entity_name = {\n",
    "    'time':['holder', 'time','name_activity', 'type_activity', \\\n",
    "                            'name_place', 'address'],\n",
    "    'name_activity':['holder','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address','reward','works'],\n",
    "    'type_activity':['holder','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address', 'works'\\\n",
    "                            , 'reward'],\n",
    "    'holder':['holder','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address'],\n",
    "    'name_place':['holder','name_place','time','name_activity', 'type_activity',  \\\n",
    "                             'address'],\n",
    "    'address':['holder','address','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place'],\n",
    "    'contact':['holder','contact', 'time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address'],\n",
    "    'works':['holder','time','name_activity', 'type_activity', 'works', \\\n",
    "                            'name_place', 'address'],\n",
    "    'register':['holder','time','name_activity', 'type_activity',  \\\n",
    "                            'name_place', 'address','register'],\n",
    "    'reward':['holder','name_activity', 'type_activity','reward','time', \\\n",
    "                            'name_place', 'address'],\n",
    "    'joiner':['time','holder','name_activity', 'type_activity','joiner', \\\n",
    "                            'name_place', 'address'\\\n",
    "                            ],\\\n",
    "    'activity':['time','holder','name_activity', 'type_activity', \\\n",
    "                            'name_place', 'address'\\\n",
    "                            ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTENT PATTERN MATCHING SIGNAL\n",
    "list_name_place_notification=[\"nơi\",\"tại đâu\",\"tại chỗ nào\",\"ở đâu\",\"chỗ nào\",\"tại đâu\",\"khu nào\",\"địa điểm nào\",\"chỗ diễn ra\",\"chỗ đâu\",\"khu tổ chức\",\"là ở\",\"là tại\",\"là nơi\",\"là tập trung tại\",\"là diễn ra tại\",\"là diễn ra ở\"]\n",
    "list_address_notification=[\"ấp nào\",\"phường nào\",\"xã nào\",\"quận nào\",\"huyện nào\",\"thành phố nào\",\"tỉnh nào\",\"đường nào\",\"đường gì\",\"số mấy\",\"địa chỉ nào\",\"địa chỉ\",\"tên đường\",\"số nhà\",\"phường mấy\",\"quận mấy\",\"số mấy\"]\n",
    "list_type_activity_notification=[\"loại hoạt động\",\"loại nào\",\"loại gì\",\"kiểu hoạt động\",\"kiểu gì\",\"kiểu nào\"]\n",
    "list_name_activity_notification=[\"tên gì\",\"tên là gì\",\"tên hoạt động là gì\",\"tên hoạt động\"] #check lai\n",
    "list_time_notification=[\"là tháng\",\"là ngày\",\"là vào\",\"tháng mấy\",\"thứ mấy\",\"là diễn ra vào\",\"ngày mấy\",\"khi nào\",\"lúc nào\",\"thời gian nào\",\"ngày nào\",\"ngày bao nhiêu\",\"giờ nào\",\"giờ bao nhiêu\",\"mấy giờ\",\"mấy h \",\"thời gian\"]\n",
    "list_holder_notification=[\"ban tổ chức\",\"btc\",\"ai tổ chức\",\"đơn vị nào tổ chức\",\"đơn vị tổ chức\",\"trường nào tổ chức\",\"clb nào tổ chức\",\"câu lạc bộ nào tổ chức\",\"người tổ chức\",\"tổ chức hả\",\"tổ chức phải không\",\"tổ chức đúng không\"]\n",
    "list_reward_notification=[\"có được drl\",'có được đrl','có được điểm rèn luyện',\"được thứ gì\",\"bao nhiêu tiền\",\"thưởng cái gì\",\"được lợi gì\",\"mấy ngày ctxh\",\"mấy điểm rèn luyện\",\"mấy drl\",\"mấy đrl\",\"mấy ngày công tác xã hội\",\"bao nhiêu ngày ctxh\",\"bao nhiêu ctxh\",\"bao nhiêu điểm rèn luyện\",\"bao nhiêu drl\",\"bao nhiêu đrl\",\"bao nhiêu ngày công tác xã hội\",\"có ích gì\",\"điểm rèn luyện\",\"được công tác xã hội\",\"được ctxh\",\"được thưởng gì\",\"được gì\",\"được cái gì\",\"có lợi gì\",\"lợi ích\",\"phần thưởng\",\"được quà gì\",\"tặng gì\",\"được bao nhiêu\",\"số tiền\",\"có được ctxh\",\"có được ngày công tác xã hội\",\"có được ngày ctxh\",\"có được tặng\",\"có được thưởng\",\"có được cho\"]\n",
    "\n",
    "\n",
    "\n",
    "#INTENT MESSAGE SIGNALS\n",
    "list_question_signal=[\" hả \",\"chứ\",\"có biết\",\"phải không\",\"là sao\",\"khi nào\",\"nơi nào\",\"không ạ\",\"k ạ\",\"là sao\",\"nữa vậy\",\"chưa á\",\"ko ạ\",\"sao ạ\",\"chưa ạ\",\"sao vậy\",\"không vậy\",\"k vậy\",\"ko vậy\",\"chưa vậy\",\" nhỉ \",\" ai\",\" ai \",\"ở đâu\",\"ở mô\",\"đi đâu\",\"bao giờ\",\"bao lâu\",\"khi nào\",\"lúc nào\",\"hồi nào\",\"vì sao\",\"tại sao\",\"thì sao\",\"làm sao\",\"như nào\",\"thế nào\",\"cái chi\",\"gì\",\"bao nhiêu\",\"?\",\" hả \",\"được không\",\"được k\",\"được ko\",\"vậy ạ\",\"nào vậy\",\"nào thế\",\"nữa không\",\"đúng không\",\"đúng k\",\"đúng ko\",\"nữa k\",\"nữa ko\",\"nào ấy\",\"nào ạ\",\"nào\",\"gì\",\"mấy\",\"đâu\",\"chứ\",\"thế\"]\n",
    "list_question_signal_last=[\"vậy\",\"chưa\",\"không\",\"sao\",\"à\",\"hả\",\"nhỉ\",\"thế\"]\n",
    "list_object=[\"bạn\",\"cậu\",\"ad\",\"anh\",\"chị\",\"admin\",\"em\",\"mày\",\"bot\"]\n",
    "list_subject=[\"mình\",\"tôi\",\"tớ\",\"tao\",\"tui\",\"anh\",\"em\"]\n",
    "list_verb_want=[\"hỏi\",\"biết\",\"xin\",\"tham gia\"]\n",
    "list_verb_have=[\"có\",\"được\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_extra_word = list_name_place_notification+list_address_notification\\\n",
    "                +list_type_activity_notification+list_name_activity_notification+list_time_notification\\\n",
    "                    +list_holder_notification+list_reward_notification+list_question_signal\n",
    "for obj in list_object:\n",
    "    list_extra_word.append(\"sao \"+obj)\n",
    "list_extra_word.append(\"còn không\")\n",
    "for subject in list_subject:\n",
    "    for verb in list_verb_want:\n",
    "        list_extra_word.extend([subject+\" muốn \"+verb,\"cho \"+subject+\" \"+verb,subject+\" cần \"+verb])\n",
    "        list_extra_word.extend([subject+\" muốn \"+verb+\" thông tin\",\"cho \"+subject+verb+\" thông tin\",subject+\" cần \"+verb+\" thông tin\"])\n",
    "for subject in list_subject:\n",
    "    list_extra_word.extend([\"chứ \"+subject,subject+\" muốn được hỏi\",subject+\" muốn được tư vấn\",subject+\" cần thông tin\",subject+\" muốn thông tin\"])\n",
    "    list_extra_word.extend([\"gửi \"+subject,\"chỉ \"+subject,\"chỉ giúp \"+subject,subject+\" muốn được hỏi thông tin\"])\n",
    "    \n",
    "for verb in list_verb_want:\n",
    "    list_extra_word.extend([\"cho \"+verb])\n",
    "for subject in list_subject:\n",
    "    list_extra_word.extend([\"cho \"+subject,\"gửi \"+subject,subject+\" cần\",subject+\" muốn\",subject+\" định\"])\n",
    "    list_extra_word.extend([\"cho \"+subject+\" thông tin\",\"gửi \"+subject+\" thông tin\",\"chỉ giúp \"+subject+\" thông tin\",subject+\" cần\",subject+\" muốn\",subject+\" định\"])\n",
    "list_extra_word.append(\"thì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_extra_word(sentence,list_extra_word):\n",
    "    for word in list_extra_word:\n",
    "        if word == \" hả \":\n",
    "            sentence = sentence.replace(word,\" ✪ \")\n",
    "        elif word == \" ai \":\n",
    "            sentence = sentence.replace(word,\" ✪ \")\n",
    "        elif word == \" ai\":\n",
    "            sentence = sentence.replace(word,\" ✪\")\n",
    "        elif word in list_question_signal_last and word == sentence.split(' ')[len(sentence.split(' '))-1]:\n",
    "            sentence = sentence.replace(word,\"✪\")\n",
    "        else:# replace la sao -> ✪ ✪\n",
    "            sentence = sentence.replace(word,' '.join(['✪']*(word.count(' ')+1)))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✪ ✪ đi mùa hè xanh'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_extra_word(\"khi nào đi mùa hè xanh\",list_extra_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digit_in_string(inputString):\n",
    "    return ''.join(['\\d' if char.isdigit() else char for char in inputString])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_last_space_list(list_input_string):\n",
    "    result = []\n",
    "    for input_string in list_input_string:\n",
    "        if input_string[len(input_string)-1]==' ':\n",
    "            result.append(input_string[:len(input_string)-1])\n",
    "            continue\n",
    "        result.append(input_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name_activity', 'type_activity', 'holder', 'time', 'name_place', 'address', 'reward', 'contact', 'register', 'works', 'joiner'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/real_dict_2000.json','r') as real_dict_file:\n",
    "    real_dict = json.load(real_dict_file)\n",
    "    list_time = real_dict['time']\n",
    "\n",
    "    print(real_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578\n"
     ]
    }
   ],
   "source": [
    "list_pattern_time = []\n",
    "list_pattern_time.extend(list(filter(hasNumbers, list_time)))\n",
    "list_pattern_time = [x.replace('\\n','') for x in list_pattern_time]\n",
    "list_pattern_time = [replace_digit_in_string(x) for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace('(','\\(').replace(')','\\)') for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace(\"hai\",\".*\").replace(\"ba\",\".*\").replace(\"tư\",\".*\").replace(\"năm\",\".*\").replace(\"sáu\",\".*\").replace(\"bảy\",\".*\") for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace('/','[/-]') for x in list_pattern_time]\n",
    "list_pattern_time.append(\"\\\\d\\\\d?h\\\\d?\\\\d?\")\n",
    "list_pattern_time.append(\"\\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d\\\\d\")\n",
    "\n",
    "list_pattern_time = [x.replace('h\\\\d\\\\d','h\\\\d?\\\\d?') for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace('\\\\dh','\\\\d\\\\d?h') for x in list_pattern_time]\n",
    "\n",
    "list_pattern_time = [x.replace('\\\\d\\\\dh','\\\\d\\\\d?h') for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace('\\\\d\\\\d[/-]','\\\\d\\\\d?[/-]') for x in list_pattern_time]\n",
    "list_pattern_time = [x.replace('[/-]\\\\d\\\\d\\\\d\\\\d','[/-]\\\\d\\\\d\\\\d?\\\\d?') for x in list_pattern_time]\n",
    "list_pattern_time.append('năm \\\\d\\\\d\\\\d\\\\d')\n",
    "list_pattern_time = list(set(list_pattern_time))\n",
    "list_pattern_time.sort(key=len,reverse=True)\n",
    "list_pattern_time.insert(0,\"ngày .* \\\\d\\\\d?h\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"thứ .* \\\\d\\\\d?h\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"ngày .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"thứ .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"cuối tháng \\\\d\\\\d?\")\n",
    "list_pattern_time.insert(0,\"đầu tháng \\\\d\\\\d?\")\n",
    "list_pattern_time.insert(0,\"ngày .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d? .* \\\\d\\\\d?h\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"thứ .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d? .* \\\\d\\\\d?h\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"từ .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d? .* đến .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "# list_pattern_time = [x+'[^ ]' for x in list_pattern_time]\n",
    "list_pattern_time.insert(0,\"\\\\d\\\\d?h\\\\d?\\\\d? - \\\\d\\\\d?h\\\\d?\\\\d? .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"thứ .* \\\\d\\\\d?h\\\\d?\\\\d? - \\\\d\\\\d?h\\\\d?\\\\d? .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"ngày .* \\\\d\\\\d?h\\\\d?\\\\d? - \\\\d\\\\d?h\\\\d?\\\\d? .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "list_pattern_time.insert(0,\"hôm .* \\\\d\\\\d?h\\\\d?\\\\d? - \\\\d\\\\d?h\\\\d?\\\\d? .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d?\\\\d?\")\n",
    "\n",
    "list_pattern_time.remove('.* \\d\\d\\d\\d')\n",
    "list_pattern_time.remove('\\d\\d ngày')\n",
    "print(len(list_pattern_time))\n",
    "# print(list_pattern_time)\n",
    "# print(\"ngày \\\\d/\\\\d/\\\\d\\\\d\\\\d\\\\d\" in list_pattern_time)\n",
    "# print(list_pattern_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pattern_reward = ['\\\\d\\\\d?\\.?\\\\d? ngày công tác xã hội','\\\\d\\\\d?\\.?\\\\d? ngày ctxh',\\\n",
    "                       '\\\\d\\\\d?\\\\d? điểm rèn luyện','\\\\d\\\\d?\\\\d? drl','\\\\d\\\\d?\\\\d? đrl',\\\n",
    "                       \"ngày công tác xã hội\",\"ngày ctxh\",\"điểm rèn luyện\",\"drl\",\"đrl\",\\\n",
    "                      \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?v?n?d\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?đồng\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?v?n?đ\",\\\n",
    "                      \n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?triệu[ ]?v?n?d\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?triệu[ ]?đồng\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?triệu[ ]?v?n?đ\",\\\n",
    "                      \n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỉ[ ]?v?n?d\",\\\n",
    "                        \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỉ[ ]?đồng\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỉ[ ]?v?n?đ\",\\\n",
    "                     \n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỷ[ ]?v?n?d\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỷ[ ]?đồng\",\\\n",
    "                       \"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?tỷ[ ]?v?n?đ\",\\\n",
    "                      \n",
    "                       \"ngày công tác xã hội\",\"ngày ctxh\",\"điểm rèn luyện\",\"đrl\",\"drl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300000000000 vnd']"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\\\d{1,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}\\.?[ ]?\\\\d{0,3}[ ]?v?n?d?\",'300000000000 vnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['từ ngày 9-5-2019 đến 9-5-2019']"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"từ .* \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d\\\\d đến \\\\d\\\\d?[/-]\\\\d\\\\d?[/-]\\\\d\\\\d\\\\d\\\\d\",'mình muốn tham gia hội thảo kĩ năng chuyên nghiệp từ ngày 9-5-2019 đến 9-5-2019 thì làm sao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entity(intent,input_sentence,list_extra_word):\n",
    "    normalized_input_sentence = compound2unicode(input_sentence)\n",
    "    normalized_input_sentence = delete_extra_word(normalized_input_sentence,list_extra_word)\n",
    "    \n",
    "    result_entity_dict={}\n",
    "    list_order_entity_name=map_intent_to_list_order_entity_name[intent]\n",
    "    print(normalized_input_sentence)\n",
    "    if 'time' in list_order_entity_name:\n",
    "        for pattern_time in list_pattern_time:\n",
    "            if re.findall(pattern_time,normalized_input_sentence)!=[]:\n",
    "                print(\"pattern_time :{0}\".format(pattern_time))\n",
    "                if 'time' not in result_entity_dict:\n",
    "                    result_entity_dict['time'] = delete_last_space_list(re.findall(pattern_time,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['time'].extend(delete_last_space_list(re.findall(pattern_time,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_time,' '.join(['✪']*(pattern_time.count(' ')+1)),normalized_input_sentence)\n",
    "        if 'time' in result_entity_dict:\n",
    "            print(result_entity_dict['time'])\n",
    "    if 'reward' in list_order_entity_name:\n",
    "        for pattern_reward in list_pattern_reward:\n",
    "            if re.findall(pattern_reward,normalized_input_sentence)!=[]:\n",
    "                print(\"pattern_reward :{0}\".format(pattern_reward))\n",
    "                if 'reward' not in result_entity_dict:\n",
    "                    result_entity_dict['reward'] = delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence))\n",
    "                else:\n",
    "                    result_entity_dict['reward'].extend(delete_last_space_list(re.findall(pattern_reward,normalized_input_sentence)))\n",
    "                \n",
    "                normalized_input_sentence = re.sub(pattern_reward,' '.join(['✪']*(pattern_reward.count(' ')+1)),normalized_input_sentence)\n",
    "        if 'reward' in result_entity_dict:\n",
    "            print(result_entity_dict['reward'])\n",
    "    matching_threshold = 0.0\n",
    "    longest_common_length, end_common_index = None, None\n",
    "    \n",
    "    map_entity_name_to_threshold={}\n",
    "    for entity_name in list_order_entity_name:\n",
    "        if entity_name in ['time','address']:\n",
    "            map_entity_name_to_threshold[entity_name]=1\n",
    "        elif entity_name in ['name_activity','contact','joiner','holder','type_activity','name_place']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['works','reward']:\n",
    "            map_entity_name_to_threshold[entity_name]=2\n",
    "        elif entity_name in ['register']:\n",
    "            map_entity_name_to_threshold[entity_name]=3\n",
    "\n",
    "\n",
    "    ordered_real_dict = OrderedDict()\n",
    "    for entity_name in map_intent_to_list_order_entity_name[intent]:\n",
    "        ordered_real_dict[entity_name] = real_dict[entity_name]\n",
    "    for entity_name, list_entity in ordered_real_dict.items():\n",
    "        print(entity_name)\n",
    "        print(\"input sentence: {0}\".format(normalized_input_sentence))\n",
    "        if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "            matching_threshold = 0.15\n",
    "        elif entity_name == \"joiner\":\n",
    "            matching_threshold = 0.2\n",
    "        else:\n",
    "            matching_threshold = 0.5\n",
    "#         print(\"000. sentence:{0}\".format(normalized_input_sentence))\n",
    "        catch_entity_threshold_loop = 0\n",
    "        while True:\n",
    "            if catch_entity_threshold_loop > 5:\n",
    "                break\n",
    "            list_dict_longest_common_entity = find_entity_longest_common(normalized_input_sentence,list_entity,entity_name)\n",
    "#             print(list_dict_longest_common_entity)\n",
    "                #     [{'longest_common_entity_index': 0,\n",
    "                #   'longest_common_length': 3,\n",
    "                #   'end_common_index': 9}]\n",
    "\n",
    "            ##find the most match longest common match (calculate by length of token match in sentence \n",
    "                                                                #/ length of entity )\n",
    "                            ##{'greatest_match_entity_index':0,'longest_common_length':3,'end_common_index':9}\n",
    "            if list_dict_longest_common_entity == []:\n",
    "                break\n",
    "            if list_dict_longest_common_entity[0]['longest_common_length'] < map_entity_name_to_threshold[entity_name] :\n",
    "                break\n",
    "            \n",
    "            list_sentence_token = normalized_input_sentence.split(' ')\n",
    "#             print(\"list_sentence_token :{0}\".format(list_sentence_token))\n",
    "            greatest_entity_index=None\n",
    "            greatest_common_length = None\n",
    "            greatest_end_common_index = None\n",
    "            max_match_entity = 0.0\n",
    "#             print(\"common entity :{0}\".format(list_dict_longest_common_entity))\n",
    "            for dict_longest_common_entity in list_dict_longest_common_entity:\n",
    "#                 print(\"0. dict_longest_common_entity: {0}\".format(dict_longest_common_entity))\n",
    "\n",
    "#                     print(\"duong\")\n",
    "#                 print(\"0.1 entity: {0}\".format(list_entity[dict_longest_common_entity['longest_common_entity_index']]))\n",
    "                longest_common_entity_index = dict_longest_common_entity['longest_common_entity_index']\n",
    "                longest_common_length = dict_longest_common_entity['longest_common_length']\n",
    "                end_common_index = dict_longest_common_entity['end_common_index']\n",
    "                list_sentence_token_match = list_sentence_token[end_common_index - longest_common_length+1:end_common_index+1]\n",
    "#                 print(\"2. list_sentence_token_match : {0}\".format(list_sentence_token_match))\n",
    "                list_temp_longest_entity_token = list_entity[longest_common_entity_index].split(' ')\n",
    "#                 print(\"3. list_temp_longest_entity_token : {0}\".format(list_temp_longest_entity_token))\n",
    "                if entity_name in [\"works\",\"register\",\"reward\"]:\n",
    "#                     print(\"list_temp_longest_entity_token :{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"list_sentence_token_match :{0}\".format(list_sentence_token_match))\n",
    "                    _,longest_common_length_entity,end_common_index_entity = lcs_length_ta(list_temp_longest_entity_token,list_sentence_token_match)\n",
    "                    list_entity_token_match = list_temp_longest_entity_token[end_common_index_entity - longest_common_length_entity +1 :end_common_index_entity +1]\n",
    "                    score = len(list_entity_token_match)/float(len(list_temp_longest_entity_token))\n",
    "#                     print(\"list_entity_token_match: {0}\".format(list_entity_token_match))\n",
    "#                     print(\"list_temp_longest_entity_token:{0}\".format(list_temp_longest_entity_token))\n",
    "#                     print(\"score :{0}\".format(score))\n",
    "                    \n",
    "                else:\n",
    "                    score = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                if score > max_match_entity:\n",
    "#                     max_match_entity = len(list_sentence_token_match)/float(len(list_temp_longest_entity_token))\n",
    "                    max_match_entity = score\n",
    "                    greatest_entity_index = longest_common_entity_index\n",
    "                    greatest_common_length = longest_common_length\n",
    "                    greatest_end_common_index = end_common_index\n",
    "#             print(list_sentence_token)\n",
    "#             print(greatest_common_length)\n",
    "#             print(greatest_end_common_index)\n",
    "#             print(\"longest_common_length: {0}\".format(longest_common_length))\n",
    "#             print(\"end_common_index: {0}\".format(end_common_index))\n",
    "#             print(\"1. greatest_common_length : {0}\".format(greatest_common_length))\n",
    "            print(max_match_entity)\n",
    "            print(\"2. greatest entity : {0}\".format(list_entity[greatest_entity_index]))\n",
    "#             print(\"2.1 greatest_end_common_index: {0}\".format(greatest_end_common_index))\n",
    "#             print(\"3. sentence match: {0}\".format(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1]))\n",
    "            if greatest_common_length >= map_entity_name_to_threshold[entity_name] and max_match_entity > matching_threshold:\n",
    "                if entity_name in ['name_activity','type_activity']:\n",
    "                    result = list_entity[greatest_entity_index]\n",
    "                else:\n",
    "                    result = ' '.join(list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1])\n",
    "                if entity_name in result_entity_dict:\n",
    "#                     result_entity_dict[entity_name].append(list_entity[greatest_entity_index])\n",
    "                    result_entity_dict[entity_name].append(result)\n",
    "                else:\n",
    "#                     result_entity_dict[entity_name] = [list_entity[greatest_entity_index]]\n",
    "                    result_entity_dict[entity_name] = [result]\n",
    "#                 list_sentence_token = list_sentence_token[:greatest_end_common_index - greatest_common_length + 1] + list_sentence_token[greatest_end_common_index +1 :]\n",
    "                list_sentence_token[greatest_end_common_index - greatest_common_length +1 :greatest_end_common_index +1] = [\"✪\"]*greatest_common_length\n",
    "                normalized_input_sentence = ' '.join(list_sentence_token)\n",
    "            catch_entity_threshold_loop = catch_entity_threshold_loop + 1\n",
    "            print(\"output sentence: {0}\".format(normalized_input_sentence))\n",
    "    return result_entity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "time\n",
      "input sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "0.3333333333333333\n",
      "2. greatest entity : kỳ nghỉ hè\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "holder\n",
      "input sentence: mình thích âm nhạc ✪ đi mùa hè xanh khoa máy tính ✪ ✪\n",
      "1.0\n",
      "2. greatest entity : khoa máy tính\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "0.1875\n",
      "2. greatest entity : ban chỉ huy chiến dịch tsmt và ban chỉ huy chiến dịch mùa hè xanh 2014\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "0.1875\n",
      "2. greatest entity : ban chỉ huy chiến dịch tsmt và ban chỉ huy chiến dịch mùa hè xanh 2014\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "0.1875\n",
      "2. greatest entity : ban chỉ huy chiến dịch tsmt và ban chỉ huy chiến dịch mùa hè xanh 2014\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "0.1875\n",
      "2. greatest entity : ban chỉ huy chiến dịch tsmt và ban chỉ huy chiến dịch mùa hè xanh 2014\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "0.1875\n",
      "2. greatest entity : ban chỉ huy chiến dịch tsmt và ban chỉ huy chiến dịch mùa hè xanh 2014\n",
      "output sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "name_activity\n",
      "input sentence: mình thích âm nhạc ✪ đi mùa hè xanh ✪ ✪ ✪ ✪ ✪\n",
      "1.0\n",
      "2. greatest entity : mùa hè xanh\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.4\n",
      "2. greatest entity : gts quà tặng âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.4\n",
      "2. greatest entity : gts quà tặng âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.4\n",
      "2. greatest entity : gts quà tặng âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.4\n",
      "2. greatest entity : gts quà tặng âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.4\n",
      "2. greatest entity : gts quà tặng âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "type_activity\n",
      "input sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.5\n",
      "2. greatest entity : lễ hội âm nhạc\n",
      "output sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "joiner\n",
      "input sentence: mình thích âm nhạc ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "0.25\n",
      "2. greatest entity : yêu thích âm nhạc và mong muốn được giúp đỡ mọi người\n",
      "output sentence: mình ✪ ✪ ✪ ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "name_place\n",
      "input sentence: mình ✪ ✪ ✪ ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n",
      "address\n",
      "input sentence: mình ✪ ✪ ✪ ✪ đi ✪ ✪ ✪ ✪ ✪ ✪ ✪ ✪\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'holder': ['khoa máy tính'],\n",
       " 'name_activity': ['mùa hè xanh'],\n",
       " 'joiner': ['thích âm nhạc']}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_entity(\"joiner\",\"mình thích âm nhạc thì đi mùa hè xanh khoa máy tính được không\",list_extra_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongCao",
   "language": "python",
   "name": "duongcao2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
